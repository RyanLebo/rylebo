---
title: "Project 2 STA 321"
author: "Ryan Lebo"
date: "2024-10-23"
output: 
  html_document:
    toc: yes
    toc_depth: 4
    toc_float: yes
    fig_width: 4
    fig_caption: yes
    number_sections: yes
    toc_collapsed: yes
    code_folding: hide
    code_download: yes
    smooth_scroll: yes
    theme: lumen
  word_document:
    toc: yes
    toc_depth: 4
    fig_caption: yes
    keep_md: yes
  pdf_document:
    toc: yes
    toc_depth: 4
    fig_caption: yes
    number_sections: yes
    fig_width: 3
    fig_height: 3
editor_options:
  chunk_output_type: inline
slways_allow_html: true
---

```{=html}

<style type="text/css">

/* Cascading Style Sheets (CSS) is a stylesheet language used to describe the presentation of a document written in HTML or XML. it is a simple mechanism for adding style (e.g., fonts, colors, spacing) to Web documents. */

h1.title {  /* Title - font specifications of the report title */
  font-size: 24px;
  color: DarkRed;
  text-align: center;
  font-family: "Gill Sans", sans-serif;
}
h4.author { /* Header 4 - font specifications for authors  */
  font-size: 20px;
  font-family: system-ui;
  color: DarkRed;
  text-align: center;
}
h4.date { /* Header 4 - font specifications for the date  */
  font-size: 18px;
  font-family: system-ui;
  color: DarkBlue;
  text-align: center;
}
h1 { /* Header 1 - font specifications for level 1 section title  */
    font-size: 22px;
    font-family: "Times New Roman", Times, serif;
    color: navy;
    text-align: center;
}
h2 { /* Header 2 - font specifications for level 2 section title */
    font-size: 20px;
    font-family: "Times New Roman", Times, serif;
    color: navy;
    text-align: left;
}

h3 { /* Header 3 - font specifications of level 3 section title  */
    font-size: 18px;
    font-family: "Times New Roman", Times, serif;
    color: navy;
    text-align: left;
}

h4 { /* Header 4 - font specifications of level 4 section title  */
    font-size: 18px;
    font-family: "Times New Roman", Times, serif;
    color: darkred;
    text-align: left;
}

body { background-color:white; }

.highlightme { background-color:yellow; }

p { background-color:white; }

</style>
```
```{r setup, include=FALSE}

if (!require("knitr")) {
   install.packages("knitr")
   library(knitr)
}
if (!require("leaflet")) {
   install.packages("leaflet")
   library(leaflet)
}
if (!require("EnvStats")) {
   install.packages("EnvStats")
   library(EnvStats)
}
if (!require("MASS")) {
   install.packages("MASS")
   library(MASS)
}
if (!require("phytools")) {
   install.packages("phytools")
   library(phytools)
}
if (!require("tidyverse")) {
   install.packages("tidyverse")
   library(tidyverse)
}
if (!require("mlbench")) {
   install.packages("mlbench")
   library(mlbench)
}
if (!require("pander")) {
   install.packages("pander")
   library(pander)
}
if (!require("pROC")) {
   install.packages("pROC")
   library(pROC)
}
knitr::opts_chunk$set(echo = FALSE,  
                   warning = FALSE,  
                                   
                   message = FALSE,  
                   results = TRUE,   
                                    
                   comment = FALSE  
                      )   
```


```{r}
bets <- read.csv("https://raw.githubusercontent.com/RyanLebo/STA-321/refs/heads/main/Project%202%20Data", header = TRUE)
wins<- bets$TeamWin
new_data <- bets %>%
  select(-OT, -Team, -OppTeam, -Datenum, -Dateslash, -Home, -TeamCov, -OUCov, -Team_id, -OppTeam_id,-TeamDiff, -TotalPts)
```


## Data and Variable Descriptions

This data set is on the NBA game betting odds and outcomes of the 2014-2015 Season. There is 1230 observations and 17 variables. The variables in this data set are 

* Datenum (categorical)- This is the amount of days since January 1, 1960
* Team (categorical)- Where the home team is from
* OppTeam (categorical)- Where the away team is from
* TeamPts (numerical)- Home team points scored
* OppPts (numerical)- Away team points scored
* Wins (binary response)- If the home team won (1 means they won, 0 means they lost)
* TeamCov (binary response) - If the home team covered the spread (1 means they covered, 0 means a "push", and -1 means they didn't cover)
* TeamSprd (numerical)- The Vegas point spread for the home team
* OvrUndr (numerical)- The over/under Vegas line for the total points in the game
* TeamDiff (numerical)- Home Points minus Away Points
* TotalPts (numerical)- Home Points plus Away Points

## Research Question
The objective of this case study is to build a logistic regression model to predict wins using various risk factors associated with the game.

## Exploratory Analysis

We first make the following pairwise scatter plots to inspect the potential issues with predictor variables.

```{r fig.align='center', fig.width=7, fig.height=7}
library(psych)
pairs.panels(new_data[,-9], 
             method = "pearson", 
             hist.col = "#00AFBB",
             density = TRUE,  
             ellipses = TRUE 
             )
```

Looking at the scatter plots we can see that none look skewed and are all unimodal besides our binary response variable which is team wins. This means that we do not need to transform any of our predictor variables. 

## Standizing Numerical Predictor Variables

Since this is a predictive model, we don't worry about the interpretation of the coefficients. The objective is to identify a model that has the best predictive performance.

```{r}

new_data$sd.TeamPts = (new_data$TeamPts-mean(new_data$TeamPts))/sd(new_data$TeamPts)
new_data$sd.OppPts = (new_data$OppPts-mean(new_data$OppPts))/sd(new_data$OppPts)
new_data$sd.TeamSprd = (new_data$TeamSprd-mean(new_data$TeamSprd))/sd(new_data$TeamSprd)
new_data$sd.OvrUndr = (new_data$OvrUndr-mean(new_data$OvrUndr))/sd(new_data$OvrUndr)

sd.new_data = new_data[, -c(1:2,4:5)]
```

## Data Split - Training and Testing Data

We randomly split the data into two subsets. 80% of the data will be used as training data. We will use the training data to search the candidate models, validate them and identify the final model using the cross-validation method. The 20% of the hold-up sample will be used for assessing the performance of the final model.

```{r}

n <- dim(sd.new_data)[1]
train.n <- round(0.8*n)
train.id <- sample(1:n, train.n, replace = FALSE)

train <- sd.new_data[train.id, ]
test <- sd.new_data[-train.id, ]

```


## Best Model Identification

In the past modules, we introduced full and reduced models to set up the scope for searching for the final model. In this case study, we use the full, reduced, and final models obtained based on the step-wise variable selection as the three candidate models.

### Cross-Validation for Model Identification

Since our training data is relatively small, I will use 5-fold cross-validation to ensure the validation data set has enough diabetes cases.

```{r}

k=5
fold.size = floor(dim(train)[1]/k)

PE1 = rep(0,5)
PE2 = rep(0,5)
PE3 = rep(0,5)
for(i in 1:k){

  valid.id = (fold.size*(i-1)+1):(fold.size*i)
  valid = train[valid.id, ]
  train.dat = train[-valid.id,]
  

  candidate01 = glm(TeamWin ~sd.TeamPts+sd.OppPts+sd.TeamSprd+sd.OvrUndr, 
                    family = binomial(link = "logit"),
                    data = train.dat)  

  candidate03 = glm(TeamWin ~ sd.TeamPts+sd.OppPts, 
                    family = binomial(link = "logit"),  
                    data = train.dat) 

   candidate02 = stepAIC(candidate01, 
                      scope = list(lower=formula(candidate03),upper=formula(candidate01)),
                      direction = "forward",   
                      trace = 0                
                      )
 
   pred01 = predict(candidate01, newdata = valid, type="response")
   pred02 = predict(candidate02, newdata = valid, type="response")
   pred03 = predict(candidate03, newdata = valid, type="response")
   
   pre.outcome01 = ifelse(as.vector(pred01) > 0.5, 1, 0)
   pre.outcome02 = ifelse(as.vector(pred02) > 0.5, 1, 0)
   pre.outcome03 = ifelse(as.vector(pred03) > 0.5, 1, 0)
   
   PE1[i] = sum(pre.outcome01 == valid$TeamWin )/length(pred01)
   PE2[i] = sum(pre.outcome02 == valid$TeamWin )/length(pred02)
   PE3[i] = sum(pre.outcome02 == valid$TeamWin )/length(pred03)
}
avg.pe = cbind(PE1 = mean(PE1), PE2 = mean(PE2), PE3 = mean(PE3))
kable(avg.pe, caption = "Average of prediction errors of candidate models")

```

The average predictive errors of both model 1 and model 2 are the same. Since model 2 is simpler than model 1, we choose model 2 as the final predictive model. This selection of the final model is based on the cut-off probability 0.5.


### Final Model Reporting

The previous cross-validation procedure identified the best model with pre-selected cut-off 0.5. The actual accuracy of the final model is given by

```{r}
pred02 = predict(candidate02, newdata = test, type="response")
pred02.outcome = ifelse(as.vector(pred02)>0.5, 1, 0)

accuracy = sum(pred02.outcome == test$TeamWin)/length(pred02)
kable(accuracy, caption="The actual accuracy of the final model")


```

Looking at these results, it tells us that this model has a 100% accuracy of predicting the outcome in the final model. 


## Conclusion

In this project we were trying to make a model that predicts wins. The final result for the accuracy of our final model was 100%. An accuracy of 100% shows that the model is identifying if an observation falls into win (1) or loss (0) for 100% of the time. 
