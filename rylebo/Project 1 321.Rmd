---
title: "Project 1"
author: "Ryan Lebo"
date: "2024-10-22"
output: 
  html_document:
    toc: yes
    toc_depth: 4
    toc_float: yes
    fig_width: 4
    fig_caption: yes
    number_sections: yes
    toc_collapsed: yes
    code_folding: hide
    code_download: yes
    smooth_scroll: yes
    theme: lumen
  word_document:
    toc: yes
    toc_depth: 4
    fig_caption: yes
    keep_md: yes
  pdf_document:
    toc: yes
    toc_depth: 4
    fig_caption: yes
    number_sections: yes
    fig_width: 3
    fig_height: 3
editor_options:
  chunk_output_type: inline
slways_allow_html: true
---



```{=html}

<style type="text/css">

/* Cascading Style Sheets (CSS) is a stylesheet language used to describe the presentation of a document written in HTML or XML. it is a simple mechanism for adding style (e.g., fonts, colors, spacing) to Web documents. */

h1.title {  /* Title - font specifications of the report title */
  font-size: 24px;
  color: DarkRed;
  text-align: center;
  font-family: "Gill Sans", sans-serif;
}
h4.author { /* Header 4 - font specifications for authors  */
  font-size: 20px;
  font-family: system-ui;
  color: DarkRed;
  text-align: center;
}
h4.date { /* Header 4 - font specifications for the date  */
  font-size: 18px;
  font-family: system-ui;
  color: DarkBlue;
  text-align: center;
}
h1 { /* Header 1 - font specifications for level 1 section title  */
    font-size: 22px;
    font-family: "Times New Roman", Times, serif;
    color: navy;
    text-align: center;
}
h2 { /* Header 2 - font specifications for level 2 section title */
    font-size: 20px;
    font-family: "Times New Roman", Times, serif;
    color: navy;
    text-align: left;
}

h3 { /* Header 3 - font specifications of level 3 section title  */
    font-size: 18px;
    font-family: "Times New Roman", Times, serif;
    color: navy;
    text-align: left;
}

h4 { /* Header 4 - font specifications of level 4 section title  */
    font-size: 18px;
    font-family: "Times New Roman", Times, serif;
    color: darkred;
    text-align: left;
}

body { background-color:white; }

.highlightme { background-color:yellow; }

p { background-color:white; }

</style>
```
```{r setup, include=FALSE}
# Detect, install, and load packages if needed.
if (!require("knitr")) {
   install.packages("knitr")
   library(knitr)
}
if (!require("leaflet")) {
   install.packages("leaflet")
   library(leaflet)
}
if (!require("EnvStats")) {
   install.packages("EnvStats")
   library(EnvStats)
}
if (!require("MASS")) {
   install.packages("MASS")
   library(MASS)
}
if (!require("phytools")) {
   install.packages("phytools")
   library(phytools)
}
#
# Specifications of outputs of code in code chunks
knitr::opts_chunk$set(echo = FALSE,  # include code chunk in the output file
                   warning = FALSE,  # Sometimes, your code may produce a warning
                                     # messages, you can choose to include the
                                     # warning messages in the output file. 
                   message = FALSE,  
                   results = TRUE,   # you can also decide whether to include 
                                     # the output in the output file.
                   comment = FALSE   # Suppress hash-tags in the output results.
                      )   
```

## Introduction
In this project we are trying to look at the research question, is there an association between greens in regulation and the predictor values availiable in the data set.

The data I am using is about statistics on lpga golfers and how they have done in the 2024 season. I got this data set off of the website (https://users.stat.ufl.edu/~winner/datasets.html).

The variables in this data set are 

* Golfer- Name of the Golfer
* Nation- Where the golfer is from
* Region- What region the golfer is from
* fairways- How many fairways the golfer hit in regulation
* fairAtt- How many attempts the golfer took to get to the fairway
* fairPct- The percent of fairways hit in regulation
* totPutts- Total amount of putts the golfer had 
* totRounds- Total amouint of rounds played by the golfer
* avePutts- Average amount of putts when you reached the green per hole
* greenReg- How many greens were hit in regulation
* totPrize- Amount of money won
* events- How many events the golfer went to
* driveDist- The average distance that the golfer hit with their drive
* sandSaves- The amount of sand saves the golfer had
* sandAtt- The amount of shots taken from the sand
* sandPct- The percentage of shots that made it out of the sand

In this data set we have sufficient information to address my research question.

## Research Question
The point of this study is to figure out the association between greens in regulation and the predictor values available in this data set. 

## Data Preperation

We are going to take out some varaibles from the model. We will start with Golfer, Nation, Region, totPrize, totRounds, and events due to these variables either being categorical or insignificant to the model. The number of events, number of rounds, and total prize will not be able to influence getting on the green in regulation. We also need to drop the variables totPutts, avePutts, sandSaves, and fairAtt because they are also not variables that can affect a persons ability to get on the green in regulation. This is due to them either being not directly correlated or they happen after you get onto the green.

```{r fig.align='center'}
lpga0 <- read.csv("https://raw.githubusercontent.com/RyanLebo/STA-321/refs/heads/main/lpga2022.csv", header = TRUE)
lpga <- lpga0[, -1]

fairway<- lpga$fairways
greens <- lpga$greenReg 


```


## Model Building

Now we need to make the full model of the data and look to see if we need to use a Box-Cox transformation on it.


```{r}
full.model = lm(greens ~ fairway+ fairPct+ driveDist+ sandAtt+ sandPct, data = lpga)
kable(summary(full.model)$coef, caption ="Regression Coefficients")

```


```{r}
par(mfrow=c(2,2))
plot(full.model)

```

In these residual plots we can see that Q-Q residual plot is positive and seems to have a normal distribution. The residuals vs fitted shows the points being to scattered and this makes the variance not constant.

Now we want to use Box-Cox to transform the data and help us find our final model.

```{r}
par(pty = "s", mfrow = c(2, 2), oma=c(.1,.1,.1,.1), mar=c(4, 0, 2, 0))
##
boxcox(greens~ fairway+ driveDist+ sandAtt+ sandPct+ log(fairPct), data = lpga, lambda = seq(-5, 10, length = 10), 
       xlab=expression(paste(lambda, ": log fairPct")))
##
boxcox(greens ~ fairway+ driveDist+ sandAtt+ sandPct+ fairPct, data = lpga, lambda = seq(-10, 10, length = 10), 
       xlab=expression(paste(lambda, ": fairPct")))
##
boxcox(greens ~ log(1+fairway)+ driveDist+ sandAtt+ sandPct+  fairPct, data = lpga, lambda = seq(-10, 10, length = 10), xlab=expression(paste(lambda, ": log-fairway")))
##
boxcox(greens ~ log(1+fairway)+ driveDist+ sandAtt+ sandPct+  log(fairPct), data = lpga, lambda = seq(-10, 10, length = 10), 
      xlab=expression(paste(lambda, ": log-fairway, log.fairPct")))
```

This box-cox transformations shows the optimal equations for the response variables. 


We perform Box-Cox transformation with square root transformed fairways hit to the nearest percent in the following.

```{r}
sqrt.fairway.log.pct = lm((greens)^0.5 ~ fairway+ driveDist+ sandAtt+ sandPct+ log(fairPct), data = lpga)
kable(summary(sqrt.fairway.log.pct)$coef, caption = "log-transformed model")

cmtrx <- summary(sqrt.fairway.log.pct)$coef
```


```{r fig.align='center', fig.height=5, fig.width=5}
par(mfrow = c(2,2))
plot(sqrt.fairway.log.pct)
```

This residual vs fitted plot starts to have the points come closer together and shows better constant variance. Also, the Q-Q plot has the points a little more closer to the line. This means that the Q-Q plot shows better normality than before and the assumption for normality is met. The assumption for constant variance is still not fully met. 

We perform Box-Cox transformation with log-transformed fairways hit to the nearest percent in the following.

```{r}
log.green = lm(log(greens) ~ fairway+ driveDist+ sandAtt+ sandPct + fairPct, data = lpga)
kable(summary(log.green)$coef, caption = "log-transform model")

```


```{r fig.align='center', fig.height=5, fig.width=5}
par(mfrow = c(2,2))
plot(log.green)
```

Looking at the residual plots above, they are similar to that of the previous model. The Q-Q plots of all three models are similar to each other, this means that the assumption of normal residuals is still satisfied for all three models. On the other hand, it seems like the plots got better for constant variance, but were not fully met for it.

Now lets look at all the Q-Q residual plots next to each other and select which one has the best model.

```{r fig.align='center', fig.width= 7, fig.height=4}

par(pty = "s", mfrow = c(1, 3))

qqnorm(full.model$residuals, main = "Full-Model")
qqline(full.model$residuals)

qqnorm(log.green$residuals, main = "Log-green")
qqline(log.green$residuals)

qqnorm(sqrt.fairway.log.pct$residuals, main = "sqrt price log pct")
qqline(sqrt.fairway.log.pct$residuals)
```

Looking at each Q-Q residual plot, I have came to the conclusion that the square root model is the best selection. This is because it has met the assumption of normalility and has the best residual plot. 

Now we should look at the goodness of fit measures to try and help find the final model.

```{r}
select=function(m){ 
 e = m$resid                         
 n0 = length(e)                        
 SSE=(m$df)*(summary(m)$sigma)^2      
 R.sq=summary(m)$r.squared             
 R.adj=summary(m)$adj.r                
 MSE=(summary(m)$sigma)^2              
 Cp=(SSE/MSE)-(n0-2*(n0-m$df))        
 AIC=n0*log(SSE)-n0*log(n0)+2*(n0-m$df)         
 SBC=n0*log(SSE)-n0*log(n0)+(log(n0))*(n0-m$df)  
 X=model.matrix(m)                     
 H=X%*%solve(t(X)%*%X)%*%t(X)         
 d=e/(1-diag(H))                       
 PRESS=t(d)%*%d   
 tbl = as.data.frame(cbind(SSE=SSE, R.sq=R.sq, R.adj = R.adj, Cp = Cp, AIC = AIC, SBC = SBC, PRD = PRESS))
 names(tbl)=c("SSE", "R.sq", "R.adj", "Cp", "AIC", "SBC", "PRESS")
 tbl
 }

```

```{r}
output.sum = rbind(select(full.model), select(sqrt.fairway.log.pct), select(log.green))
row.names(output.sum) = c("full.model", "ssqrt.fairway.log.pct", "log.green")
kable(output.sum, caption = "Goodness-of-fit Measures of Candidate Models")
```

We have a sample size of 158 which is large. We can see from the above table that the goodness-of-fit measures of the first model are better than the other two models. Considering the interpretability, goodness-of-fit, and simplicity, we choose the second model as the final model. This is because the residual plots for the second model had better constant variance and the goodness-of-fit measurements were very close between all models.


## Bootstrap

Here we will use the bootstrap method to get a confidence interval of the coefficients in our selected model. 

```{r}
sqrt.fairway.log.pct = lm((greens)^0.5 ~ fairway+ driveDist+ sandAtt+ sandPct+ log(fairPct), data = lpga)

B = 1000      

para_sqrt = dim(model.frame(sqrt.fairway.log.pct))[2]  
samp_sqrt = dim(model.frame(sqrt.fairway.log.pct))[1] 
coef.mtrx = matrix(rep(0, B*para_sqrt), ncol = para_sqrt)       

for (i in 1:B){
  bootc.id = sample(1:samp_sqrt, samp_sqrt, replace = TRUE) 
  sqrt.green.bt =lm((greens)^0.5 ~ fairway+ driveDist+ sandAtt+ sandPct+ log(fairPct), data = lpga[bootc.id,])
  
  coef.mtrx[i,] = coef(sqrt.green.bt)    
}
```


```{r}
boot_hist = function(log_trx, bt.coef.mtrx, var.id, var.nm){
 
  x1.1 <- seq(min(bt.coef.mtrx[,var.id]), max(bt.coef.mtrx[,var.id]), length=300 )
  y1.1 <- dnorm(x1.1, mean(bt.coef.mtrx[,var.id]), sd(bt.coef.mtrx[,var.id]))

  highestbar = max(hist(bt.coef.mtrx[,var.id], plot = FALSE)$density) 
  ylimit <- max(c(y1.1,highestbar))
  hist(bt.coef.mtrx[,var.id], probability = TRUE, main = var.nm, xlab="", 
       col = "azure1",ylim=c(0,ylimit), border="lightseagreen")
  lines(x = x1.1, y = y1.1, col = "red3")
  lines(density(bt.coef.mtrx[,var.id], adjust=2), col="blue") 

}
```

We will now make visual representations of histograms for each of the regression coefficients in the final model.

```{r fig.align='center', fig.width=7, fig.height=5}
par(mfrow=c(2,3))  
boot_hist(bt.coef.mtrx=coef.mtrx, var.id=1, var.nm ="Intercept" )
boot_hist(bt.coef.mtrx=coef.mtrx, var.id=2, var.nm ="Fairway" )
boot_hist(bt.coef.mtrx=coef.mtrx, var.id=3, var.nm ="Drive Distance" )
boot_hist(bt.coef.mtrx=coef.mtrx, var.id=4, var.nm ="Sand Attempts" )
boot_hist(bt.coef.mtrx=coef.mtrx, var.id=5, var.nm ="Sand Percent" )
boot_hist(bt.coef.mtrx=coef.mtrx, var.id=6, var.nm ="Fairway Percent" )

```

Since both of the density curves in the histograms are close together, we can conclude that the bootstrap confidence intervals will be consistent with the significance tests.

The code below will get a 95% bootstrap confidence interval for the final model.

```{r}
para_sqrt = dim(coef.mtrx)[2]  
boot_conf = NULL
boot_wd = NULL
for (i in 1:para_sqrt){
  low_conf = round(quantile(coef.mtrx[, i], 0.025, type = 2),8)
  up_conf = round(quantile(coef.mtrx[, i],0.975, type = 2 ),8)
  boot_wd[i] =  up_conf - low_conf
  boot_conf[i] = paste("[", round(low_conf,4),", ", round(up_conf,4),"]")
 }

kable(as.data.frame(cbind(formatC(cmtrx,4,format="f"), boot_conf.95=boot_conf)), 
      caption = "Bootstrap CI")
```

We can see that since some confidence intervals contain 0 that the intervals are not consistent.

## Residual Bootstrap

Below is a histogram that shows the distribution of the bootstrap residuals.

```{r fig.align='center', fig.width=7, fig.height=4}
hist(sort(sqrt.fairway.log.pct$residuals),n=40,
     xlab="Residuals",
     col = "lightblue",
     border="red",
     main = "Histogram of Bootstrap Residuals")
```

Looking at the histogram you can see that it is slightly left skewed and there is one outlier on the far left.


```{r}
sqrt.fairway.log.pct = lm((greens)^0.5 ~ fairway+ driveDist+ sandAtt+ sandPct+ log(fairPct), data = lpga)

model_resid = sqrt.fairway.log.pct$residuals

B = 1000      

para_sqrt = dim(model.matrix(sqrt.fairway.log.pct))[2]  
samp_sqrt = dim(model.matrix(sqrt.fairway.log.pct))[1] 
btr.mtrx = matrix(rep(0, para_sqrt * B), ncol = para_sqrt)

for (i in 1:B){
   bt.sq.green = sqrt.fairway.log.pct$fitted.values +
        sample(sqrt.fairway.log.pct$residuals, samp_sqrt, replace = TRUE) 
   btr.model = lm(bt.sq.green ~fairway+ driveDist+ sandAtt+ sandPct+ log(fairPct), data = lpga) 
  btr.mtrx[i,]=btr.model$coefficients 
}
```


We must make histograms to show the residual bootstrap estimates.

```{r}
boot.hist = function(bt.coef.mtrx, var.id, var.nm){
 
  x1.1 <- seq(min(bt.coef.mtrx[,var.id]), max(bt.coef.mtrx[,var.id]), length=300 )
  y1.1 <- dnorm(x1.1, mean(bt.coef.mtrx[,var.id]), sd(bt.coef.mtrx[,var.id]))

  highestbar = max(hist(bt.coef.mtrx[,var.id], plot = FALSE)$density) 
  ylimit <- max(c(y1.1,highestbar))
  hist(bt.coef.mtrx[,var.id], probability = TRUE, main = var.nm, xlab="", 
       col = "azure1",ylim=c(0,ylimit), border="lightseagreen")
  lines(x = x1.1, y = y1.1, col = "red3")
  lines(density(bt.coef.mtrx[,var.id], adjust=2), col="blue") 

}
```



```{r fig.align='center', fig.width=7, fig.height=5}
par(mfrow=c(2,3))  
boot.hist(bt.coef.mtrx=btr.mtrx, var.id=1, var.nm ="Intercept" )
boot.hist(bt.coef.mtrx=btr.mtrx, var.id=2, var.nm ="Fairway" )
boot.hist(bt.coef.mtrx=btr.mtrx, var.id=3, var.nm ="Drive Distance" )
boot.hist(bt.coef.mtrx=btr.mtrx, var.id=4, var.nm ="Sand Attempts" )
boot.hist(bt.coef.mtrx=btr.mtrx, var.id=5, var.nm ="Sand Percent" )
boot.hist(bt.coef.mtrx=btr.mtrx, var.id=6, var.nm ="Fairway Percent" )


```

Looking at the histograms the density curves in the histograms are close together, we can conclude that the bootstrap confidence intervals will be consistent with the significance tests.

The 95% residual bootstrap confidence interval is shown below.

```{r}

para_sqrt = dim(coef.mtrx)[2]  
boot_conf2 = NULL
boot_wd2 = NULL
for (i in 1:para_sqrt){
  low_conf = round(quantile(btr.mtrx[, i], 0.025, type = 2),8)
  up_conf = round(quantile(btr.mtrx[, i],0.975, type = 2 ),8)
  boot_wd2[i] = up_conf - low_conf
  boot_conf2[i] = paste("[", round(low_conf,4),", ", round(up_conf,4),"]")
}

kable(as.data.frame(cbind(formatC(cmtrx,4,format="f"), boot_conf.95=boot_conf2)), 
      caption = "Regression Matrix with a 95% Residual Bootstrap CI")
```

The residual bootstrap confidence intervals have the same results as p-values do beisdes the intercept since it contains 0 in the confidence interval. This is because the sample size is large enough so that the sampling distributions of estimated coefficients have sufficiently good approximations of normal distributions.


## Combining Results

Finally, we put all inferential statistics in a single table so we can compare these results.

```{r}
kable(as.data.frame(cbind(formatC(cmtrx[,-3],4,format="f"), btc.ci.95=boot_conf,btr.ci.95=boot_conf2)), 
      caption="Final Combined Inferential Statistics")
```

This table shows the results side by side of the two bootstrap confidence intervals.


```{r}
kable(round(cbind(boot_wd, boot_wd2),4), caption="width of the two bootstrap confidence intervals")
```

Looking at this table you can see that the widths of each are mostly similar to each other.


## Summary and Discussion

The best model was the square root model. This is because the residual plots for them show more normality than the other models, better constant variance, and no multicollinearity. They also have close goodness-of-fit measures to the other models.

Looking at the response variable we can see most variables besides fairways hit contain 0 in the confidence interval for the combined inferential statistics. This shows that fairways hit is the most statistically significant variable in comparison to greens in regulation.

I had no drawbacks or improvements I can think of.

In the future I will use total prize as my response variable. This is because you can see then what statistic in golf is the most important factor in how much you win in games.




